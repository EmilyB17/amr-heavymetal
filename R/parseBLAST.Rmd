---
title: "BLAST Output Parsing"
author: "Emily Bean"
date: "3/11/2020"
output: 
  rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview

This script parses the output from the custom BLAST alignment and combines all samples into one spreadsheet. Data has been downloaded locally from PSU ACI-ICDS storage and runs from a local repo clone directory. the BlAST output text files are about 5GB.

```{r}

require(dplyr)
require(stringr)
require(tibble)

# set working directory to local Github repo clone
PATH = "C:/Users/emily/AppData/Local/Packages/CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc/LocalState/rootfs/home/emily/blasted"

# read MEGARES external DB key
# read in annotations CSV
#ids <- read.csv(paste0(PATH, "data/megares_to_external_header_mappings_v2.00.csv"),             stringsAsFactors = FALSE)


# get files from the path 
files <- list.files(PATH, full.names = TRUE)

# error check: is each file unique (no duplicates)?
if(!length(unique(files)) == length(files)) {
  
  stop("There are duplicate files")
  
}

# read in sequencing depth of each sample
seqs <- read.table("https://raw.githubusercontent.com/EmilyB17/amr-brazil/master/data/sampleDepth.txt", sep = "\t", header = TRUE, stringsAsFactors = FALSE)

```


A loop reads in each file and wrangles it to get the counts for each gene. This is computationally expensive and not ideal to run in RStudio.

Output is two dataframes: `countsDF` holds gene counts for each unique gene and sample, with total number of hits for that sample (for downstream normalization). `lengthDF` holds the length of each megID gene. 

```{r}

# create empty dataframe to fill with gene counts
countsDF <- data.frame()
# create empty dataframe to fill with gene length
lengthDF <- data.frame()

# read in each file and parse
for(i in 1:length(files)) {
  
  # read the file
  dat <- read.table(files[i], header = FALSE, stringsAsFactors = FALSE, sep = "\t")
  
  # assign column names
  colnames(dat) <- c("readID", "megID", "pident", "slen", "qcovs", "qcovhsp", "qcovus")
  
  # parse subject length
  slens <- dat %>% 
    select(megID, slen) %>% 
    # get the minimum gene length for each megID
    group_by(megID) %>% 
    summarize(minlen = min(slen))
  
  # get subject length (gene length) for each megID
  megdat <- dat %>% 
    group_by(megID) %>% 
    summarize(idcount = n()) %>% 
    right_join(slens, by = "megID") %>% 
    # add sample name
    mutate(sample = files[i])
  
  
  # group by megID and then by pattern
  sumdat <- megdat %>% 
    # get number of megID hits with each gene length
    group_by(megID, minlen) %>% 
    summarize(count = sum(idcount)) %>% 
    ungroup() %>% 
    # get the pattern without megID
    # this is an important grouping variable since multiple megIDs map to a single gene
    mutate(pattern = factor(sapply(strsplit(megID, "MEG_[0-9]+\\|"), `[`, 2))) %>% 
    # group by pattern
    group_by(pattern) %>% 
    summarize(genecount = sum(count),
              genelen = min(minlen)) %>% 
    # add sample name and read length
    mutate(nhits = nrow(dat),
           sample = files[i])
  
  # append to output file
  countsDF <- rbind(sumdat, countsDF)
  lengthDF <- rbind(megdat, lengthDF)
  
  # print progress report
  cat("\n finished parsing", files[i], "... \n")
  
}

```


To normalize by sequencing depth, we need number of reads for each sample. This was done on the command line with `seqkit stats` on the merged reads and written to text file. Here we will briefly parse it to work with. 

```{#r}

# read merged stats df
seqs <- read.table(paste0(PATH, "data/mergestats.txt"), stringsAsFactors = FALSE, header = TRUE)  %>% 
  filter(!file == "file") 
seqs <- seqs %>% 
  mutate(name = sapply(strsplit(sapply(strsplit(seqs$file, "/"), `[`, 7), ".extendedFrags.fastq"), `[`, 1)) %>% 
  select(name, num_seqs) 

# convert character vector to integer
seqs$length <- as.integer(str_remove_all(seqs$num_seqs, ","))
seqs$num_seqs <- NULL

# write to table

```

Subset data to get only the number of gene alignments for each sample

```{r}

# get only number of hits per sample
hits <- countsDF %>% select(sample, nhits) %>% 
  mutate(name = sapply(strsplit(sapply(strsplit(countsDF$sample, "/"), `[`, 13), ".txt"), `[`, 1)) %>% 
  select(-sample) %>% 
  group_by(name) %>% 
  distinct() %>% 
  ungroup()

```

There were several control samples and a sample from another study ("Alien") added. 

```{r}
# pick out control samples
controls <- paste("G527_94_NoTemplate-DNAextraction2_S94",
                  "G527_96_NoTemplate-LibraryPrep_S96",
                  "G527_95_MockCommunity_S95",
                  "G527_93_NoTemplate-DNAextraction1_S93",
                  "G527_49_2019_DNA_S49", sep = "|")

# get dataframe without controls or Alien sample
nocontrols <- countsDF %>% 
  mutate(name = sapply(strsplit(sapply(strsplit(countsDF$sample, "/"), `[`, 13), ".txt"), `[`, 1)) %>% 
  filter(!str_detect(name, controls))

# get dataframe of controls only
controlsDF <- countsDF %>% 
  mutate(name = sapply(strsplit(sapply(strsplit(countsDF$sample, "/"), `[`, 13), ".txt"), `[`, 1)) %>% 
  filter(str_detect(name, controls)) %>% 
  # remove Alien sample
  filter(!str_detect(name, "G527_49_2019_DNA_S49"))

```
